---
title: rke2203版本安装
date: 2022-06-09 10:25:43
tags:
---

企业版基于Helm的部署文档

<!-- more -->

- ## 部署 K8s 集群

  - 安装 docker

    ```bash
    #安装节点执行
    ssh-keygen
    #下载安装包
    https://goodrain-delivery.oss-cn-hangzhou.aliyuncs.com/rke   
    wget https://rainbond-pkg.oss-cn-shanghai.aliyuncs.com/offline/5.3-enterprise/rainbond-offline-enterprise-2203.tgz
    #所有节点执行
    tar xvf rainbond-offline-enterprise-2203.tgz && cd offline
    # SSH_RSA的值可以在安装节点通过 cat /root/.ssh/id_rsa.pub 获取（三台都输入管理节点的值）
    export SSH_RSA='ssh-rsa （红色字体请替换成主节点的值） AAAAB3NzaC1yc2EAAAADAQABAAABAQCkA4iSDTve4xPy8GMkwC5JI6S/DVKYt5Q2IGL5CX0oeWsTgPnmC743uLIgTrwT6vlaR95Bb31CxsGC//3x/YikEEziPxxopFD24L5sqzw+jG8R8kRhFT9G5OPDgFZwiUrsjUbFl13Vl60OqfZIjGihQOYgQvz2pVfUy/eopdDTbTTNc/clZBV1KoV7pPRdmJfNPS2nHCETT0K34JsE03J231Lpp1NKmK5XTFs+oas8oDR8+zdnwU631GchkEqSlO6NygaY5B7uSpOsLb8FugNtYyZ12qUw94FJZcF7ipPwjEGojXyG9p+eIn+bRAmE0OpagoKqASqn5EPuKjQa2ppv root@node' && ./init_node_offline.sh
    ```

  - 安装 k8s

    - 修改 cluster.yaml 文件内容，根据需要添加其它节点

      ```yaml
      # If you intended to deploy Kubernetes in an air-gapped environment,
      # please consult the documentation on how to configure custom RKE images.
      nodes:
      - address: 192.168.2.212       #主节点的IP
        port: "22"
        internal_address: 192.168.2.212    ##主节点的IP
        role:
        - controlplane
        - worker
        - etcd
        hostname_override: ""
        user: docker
        docker_socket: /var/run/docker.sock
        ssh_key: ""
        ssh_key_path: ~/.ssh/id_rsa
        ssh_cert: ""
        ssh_cert_path: ""
        labels: {}
        taints: []
      - address: 192.168.2.113     #从节点的IP
        port: "22"
        internal_address: 192.168.2.113     #从节点的IP
        role:
        - worker
        hostname_override: ""
        user: docker
        docker_socket: /var/run/docker.sock
        ssh_key: ""
        ssh_key_path: ~/.ssh/id_rsa
        ssh_cert: ""
        ssh_cert_path: ""
        labels: {}
        taints: []
      services:
        etcd:
          image: ""
          extra_args: {}
          extra_binds: []
          extra_env: []
          win_extra_args: {}
          win_extra_binds: []
          win_extra_env: []
          external_urls: []
          ca_cert: ""
          cert: ""
          key: ""
          path: ""
          uid: 0
          gid: 0
          snapshot: null
          retention: ""
          creation: ""
          backup_config: null
        kube-api:
          image: ""
          extra_args: {}
          extra_binds: []
          extra_env: []
          win_extra_args: {}
          win_extra_binds: []
          win_extra_env: []
          service_cluster_ip_range: 10.43.0.0/16
          service_node_port_range: ""
          pod_security_policy: false
          always_pull_images: false
          secrets_encryption_config: null
          audit_log: null
          admission_configuration: null
          event_rate_limit: null
        kube-controller:
          image: ""
          extra_args: {}
          extra_binds: []
          extra_env: []
          win_extra_args: {}
          win_extra_binds: []
          win_extra_env: []
          cluster_cidr: 10.42.0.0/16
          service_cluster_ip_range: 10.43.0.0/16
        scheduler:
          image: ""
          extra_args: {}
          extra_binds: []
          extra_env: []
          win_extra_args: {}
          win_extra_binds: []
          win_extra_env: []
        kubelet:
          image: ""
          extra_args: {}
          extra_binds: []
          extra_env: []
          win_extra_args: {}
          win_extra_binds: []
          win_extra_env: []
          cluster_domain: cluster.local
          infra_container_image: ""
          cluster_dns_server: 10.43.0.10
          fail_swap_on: false
          generate_serving_certificate: false
        kubeproxy:
          image: ""
          extra_args: {}
          extra_binds: []
          extra_env: []
          win_extra_args: {}
          win_extra_binds: []
          win_extra_env: []
      network:
        plugin: flannel
        options: {}
        mtu: 0
        node_selector: {}
        update_strategy: null
        tolerations: []
      authentication:
        strategy: x509
        sans: []
        webhook: null
      addons: ""
      addons_include: []
      system_images:
        etcd: goodrain.me/coreos-etcd:v3.4.13-rke
        alpine: goodrain.me/rke-tools:v0.1.68
        nginx_proxy: goodrain.me/rke-tools:v0.1.68
        cert_downloader: goodrain.me/rke-tools:v0.1.68
        kubernetes_services_sidecar: goodrain.me/rke-tools:v0.1.68
        kubedns: goodrain.me/k8s-dns-kube-dns:1.15.10
        dnsmasq: goodrain.me/k8s-dns-dnsmasq-nanny:1.15.10
        kubedns_sidecar: goodrain.me/k8s-dns-sidecar:1.15.10
        kubedns_autoscaler: goodrain.me/cluster-proportional-autoscaler:1.8.1
        coredns: goodrain.me/coredns-coredns:1.7.0
        coredns_autoscaler: goodrain.me/cluster-proportional-autoscaler:1.8.1
        nodelocal: goodrain.me/k8s-dns-node-cache:1.15.13
        kubernetes: goodrain.me/hyperkube:v1.19.6-rke
        flannel: goodrain.me/coreos-flannel:v0.13.0-rke
        flannel_cni: goodrain.me/flannel-cni:v0.3.0-rke
        calico_node: goodrain.me/calico-node:v3.16.5
        calico_cni: goodrain.me/calico-cni:v3.16.5
        calico_controllers: goodrain.me/calico-kube-controllers:v3.16.5
        calico_ctl: goodrain.me/calico-ctl:v3.16.5
        calico_flexvol: goodrain.me/calico-pod2daemon-flexvol:v3.16.5
        canal_node: goodrain.me/calico-node:v3.16.5
        canal_cni: goodrain.me/calico-cni:v3.16.5
        canal_controllers: goodrain.me/calico-kube-controllers:v3.16.5
        canal_flannel: goodrain.me/coreos-flannel:v0.13.0-rancher1
        canal_flexvol: goodrain.me/calico-pod2daemon-flexvol:v3.16.5
        weave_node: weaveworks/weave-kube:2.7.0
        weave_cni: weaveworks/weave-npc:2.7.0
        pod_infra_container: goodrain.me/pause:3.2
        ingress: goodrain.me/nginx-ingress-controller:nginx-0.35.0-rancher2
        ingress_backend: goodrain.me/nginx-ingress-controller-defaultbackend:1.5-rancher1
        metrics_server: goodrain.me/metrics-server:v0.3.6
        windows_pod_infra_container: goodrain.me/kubelet-pause:v0.1.4
        aci_cni_deploy_container: noiro/cnideploy:5.1.1.0.1ae238a
        aci_host_container: noiro/aci-containers-host:5.1.1.0.1ae238a
        aci_opflex_container: noiro/opflex:5.1.1.0.1ae238a
        aci_mcast_container: noiro/opflex:5.1.1.0.1ae238a
        aci_ovs_container: noiro/openvswitch:5.1.1.0.1ae238a
        aci_controller_container: noiro/aci-containers-controller:5.1.1.0.1ae238a
        aci_gbp_server_container: noiro/gbp-server:5.1.1.0.1ae238a
        aci_opflex_server_container: noiro/opflex-server:5.1.1.0.1ae238a
      ssh_key_path: ~/.ssh/id_rsa
      ssh_cert_path: ""
      ssh_agent_auth: false
      authorization:
        mode: rbac
        options: {}
      ignore_docker_version: null
      kubernetes_version: ""
      private_registries: []
      ingress:
        provider: "none"
        options: {}
        node_selector: {}
        extra_args: {}
        dns_policy: ""
        extra_envs: []
        extra_volumes: []
        extra_volume_mounts: []
        update_strategy: null
        http_port: 0
        https_port: 0
        network_mode: ""
        tolerations: []
        default_backend: null
      cluster_name: ""
      cloud_provider:
        name: ""
      prefix_path: ""
      win_prefix_path: ""
      addon_job_timeout: 0
      bastion_host:
        address: ""
        port: ""
        user: ""
        ssh_key: ""
        ssh_key_path: ""
        ssh_cert: ""
        ssh_cert_path: ""
      monitoring:
        provider: ""
        options: {}
        node_selector: {}
        update_strategy: null
        replicas: null
        tolerations: []
      restore:
        restore: false
        snapshot_name: ""
      rotate_encryption_key: false
      dns: null
      
      ```

    - 执行安装操作

      ```bash
      ./rke up
      #执行成功后使用不了kubectl命令，所以需要创建目录，并拷贝配置文件
      mkdir /root/.kube && cp kube_config_cluster.yml /root/.kube/config
      #启动后metrics-server会获取不到镜像，修改镜像拉取策略为 IfNotPresent
      kubectl edit deploy -n kube-system metrics-server
      ```

    - 确认安装结果

      ```bash
      kubectl get node
      ```

- ## 部署Mysql主从集群

  在主从服务器中执行以下命令获取镜像：

  ```bash
  docker pull mysql
  ```

  容器启动时需要分别挂载主从数据库的配置文件

  主数据库

  ```bash
  $ vi /var/lib/mysql/my.cnf
  [mysqld]
  pid-file        = /var/run/mysqld/mysqld.pid
  socket          = /var/run/mysqld/mysqld.sock
  datadir         = /var/lib/mysql
  secure-file-priv= NULL
  # Disabling symbolic-links is recommended to prevent assorted security risks
  symbolic-links=0
  # 服务端默认utf8编码
  character-set-server=utf8mb4
  # 默认存储引擎
  default-storage-engine=INNODB
  
  # 主从配置
  log-bin=binlog
  server-id=121
  gtid-mode=on
  enforce-gtid-consistency=on
  log-slave-updates=on
  expire_logs_days=14
  
  # Compatible with versions before 8.0
  default_authentication_plugin=mysql_native_password
  skip-host-cache
  skip-name-resolve
  
  [client]
  #设置客户端编码
  default-character-set=utf8mb4
  [mysql]
  # 设置mysql客户端默认编码
  default-character-set=utf8mb4
  
  # Custom config should go here
  !includedir /etc/mysql/conf.d/
  # Custom config should go here
  !includedir /etc/mysql/conf.d/
  ```

  从数据库

  ```bash
  $ vi /var/lib/mysql/my.cnf
  [mysqld]
  pid-file        = /var/run/mysqld/mysqld.pid
  socket          = /var/run/mysqld/mysqld.sock
  datadir         = /var/lib/mysql
  secure-file-priv= NULL
  # Disabling symbolic-links is recommended to prevent assorted security risks
  symbolic-links=0
  # 服务端默认utf8编码
  character-set-server=utf8mb4
  # 默认存储引擎
  default-storage-engine=INNODB
  
  # 主从配置
  server-id=122
  gtid-mode=on
  enforce-gtid-consistency=on
  log-slave-updates=on
  expire_logs_days=14
  
  # Compatible with versions before 8.0
  default_authentication_plugin=mysql_native_password
  skip-host-cache
  skip-name-resolve
  
  [client]
  #设置客户端编码
  default-character-set=utf8mb4
  [mysql]
  # 设置mysql客户端默认编码
  default-character-set=utf8mb4
  
  # Custom config should go here
  !includedir /etc/mysql/conf.d/
  # Custom config should go here
  !includedir /etc/mysql/conf.d/
  ```

  启动数据库

  ```bash
  docker run --name mysql_master --restart=always \
  -p 3306:3306 \
  -v /var/lib/mysql/my.cnf:/etc/mysql/my.cnf \
  -v /var/lib/mysql/data:/var/lib/mysql \
  -e MYSQL_ROOT_PASSWORD=eed1eu.s0S \
  -d mysql
  ```

  查看数据库字符编码,并创建用户授权

  ```mysql
  # 进入数据库
  docker exec -it mysql_master bash
  # 创建用户授权
  mysql>  CREATE USER 'slave'@'%' IDENTIFIED WITH mysql_native_password BY 'slave';
  mysql>  GRANT REPLICATION SLAVE ON *.* TO 'slave'@'%';
  mysql>  flush privileges;
  ```

  获取主节点当前binary log文件名和位置（position）

  ```mysql
  mysql>  SHOW MASTER STATUS;
  +---------------+----------+--------------+------------------+------------------------------------------+
  | File          | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set                        |
  +---------------+----------+--------------+------------------+------------------------------------------+
  | binlog.000003 |      868 |              |                  | 1b009ef8-a67f-11ea-8c9a-0242ac110002:1-8 |
  +---------------+----------+--------------+------------------+------------------------------------------+
  1 row in set (0.00 sec)
  ```

  启动数据库

  ```bash
  docker run --name mysql_slave --restart=always \
  -p 3306:3306 \
  -v /var/lib/mysql/my.cnf:/etc/mysql/my.cnf \
  -v /var/lib/mysql/data:/var/lib/mysql \
  -e MYSQL_ROOT_PASSWORD=eed1eu.s0S \
  -d mysql
  ```

  配置主从复制

  ```mysql
  # 进入数据库
  docker exec -it mysql_slave bash
  # 主从配置
  mysql>  CHANGE MASTER TO
  mysql>  MASTER_HOST='192.168.0.162',
  mysql>  MASTER_USER='slave',
  mysql>  MASTER_PASSWORD='slave',
  mysql>  MASTER_PORT=3306,
  mysql>  MASTER_LOG_FILE='binlog.000003',
  mysql>  MASTER_LOG_POS=868;
  
  # 开启主从同步
  mysql>  start slave;
  # 再查看主从同步状态
  mysql>  show slave status;
  ```

  这里只要看到两个参数Slave_IO_Running和Slave_SQL_Running都为true且Error字段都为空则代码主从正常复制

  在主服务器创建Rainbond部署所需的数据库，查看从服务器是否同步更新了数据

  在主库创建库

  ```mysql
  mysql>  create database console;
  mysql>  create database region;
  ```

  在从库查看

  ```mysql
  mysql>  show databases;
  +--------------------+
  | Database           |
  +--------------------+
  | information_schema |
  | console            |
  | mysql              |
  | performance_schema |
  | region             |
  | sys                |
  +--------------------+
  5 rows in set (0.00 sec)
  ```

  数据同步成功，则主从复制部署完成

  在主库创建用户并授权，为Rainbond后续部署做准备

  ```mysql
  mysql>  CREATE USER rainbond;
  mysql>  ALTER USER 'rainbond'@'%' IDENTIFIED WITH mysql_native_password BY 'Gz1ea3.G';
  mysql>  GRANT ALL PRIVILEGES ON *.* TO 'rainbond'@'%';
  ```

  从节点配置备份任务

  编写备份脚本

  ```bash
  $  mkdir -p /var/lib/mysql/backup
  $  vi /var/lib/mysql/backup/mysql-backup.sh
  #!/bin/bash
  DATE=`date +%Y%m%d%H%M`
  DB_USER=rainbond  #数据库用户名
  DB_PASS="Gz1ea3.G"     #数据库密码
  BACKUP=/var/lib/mysql/backup/   #备份文件存储路径
        
  #备份
  
  /usr/bin/mysqldump -u$DB_USER -p$DB_PASS -h 127.0.0.1 |gzip > ${BACKUP}\/rainbond_${DATE}.sql.gz
        
  #保留最近30天的备份文件
  
  find ${BACKUP} -name "rainbond_*.sql.gz" -type f -mtime +30 -exec rm {} \; > /dev/null 2>&1
  ```

  配置计划任务

  ```bash
  $ crontab -e
  0 3 * * * /var/lib/mysql/backup/mysql-backup.sh
  ```

  赋予执行权限

  ```bash
  chmod +x /var/lib/mysql/backup/mysql-backup.sh
  ```

- ## 部署gfs集群

  通过 Kubernetes 安装 Glusterfs 集群

  - 安装好的 Kubernetes 集群，其中三个节点应各挂载一块空间不小于 500G 的 SSD 磁盘

  - 将准备好的磁盘格式化并挂载到指定目录

    ```bash
    # 查看可用磁盘
    fdisk -l
    # 分区并格式化
    mkfs.xfs  /dev/vdb1
    mkdir  -p /data
    echo "/dev/vdb1  /data  xfs  defaults 1 2" >>/etc/fstab
    # 挂载
    mount -a
    # 确定/data挂载
    df -h | grep data
    ```

  - 在 Kubernetes 所有节点安装对应版本的 Glusterfs 客户端工具并加载所需内核模块

    - Ubuntu 1604/1804

    ```bash
    apt install software-properties-common
    add-apt-repository ppa:gluster/glusterfs-7
    apt update
    apt install glusterfs-client -y
    modprobe dm_thin_pool
    ```

    - CentOS 7

    ```bash
    yum -y install centos-release-gluster
    yum -y install glusterfs-client
    modprobe dm_thin_pool
    ```

  部署 Glusterfs 集群

  以下操作在 Kubernetes 的任一 master 节点执行一次即可

  - 获取对应的项目

    ```bash
    git clone https://gitee.com/liu_shuai2573/gfs-k8s.git && cd gfs-k8s
    ```

  - 设置节点标签，指定对应节点运行 Glusterfs 组件

    ```bash
    #设置标签，将 Glusterfs1/2/3 更改为对应的 Kubernetes node 节点
    kubectl label node Glusterfs1 Glusterfs2 Glusterfs3 storagenode=glusterfs
    #执行此操作后，对应节点将只运行 Glusterfs 服务，如需复用该节点，请不要执行此操作
    kubectl taint node Glusterfs1 Glusterfs2 Glusterfs3 glusterfs=true:NoSchedule
    ```

  - 创建 Glusterfs 服务

    ```bash
    kubectl create -f gluster-daemonset.yaml
    ```

  - 检查 Glusterfs 服务是否在指定节点正常运行

    ```bash
    kubectl get pods -o wide --selector=glusterfs-node=daemonset
    NAME              READY   STATUS    RESTARTS   AGE    IP              NODE            NOMINATED NODE   READINESS GATES
    glusterfs-2k5rm   1/1     Running   0          52m    192.168.2.200   192.168.2.200   <none>           <none>
    glusterfs-mc6pg   1/1     Running   0          134m   192.168.2.22    192.168.2.22    <none>           <none>
    glusterfs-tgsn7   1/1     Running   0          134m   192.168.2.224   192.168.2.224   <none>           <none>
    ```

  - 将 Glusterfs 服务添加为统一集群

    ```bash
    #通过其中一个 Glusterfs 服务将其他两个 Glusterfs 服务加入集群
    kubectl exec -ti glusterfs-2k5rm gluster peer probe Glusterfs2_IP
    kubectl exec -ti glusterfs-2k5rm gluster peer probe Glusterfs3_IP
    #检测是否添加成功，添加成功会显示其他两个 Glusterfs 服务的状态
    kubectl exec -ti glusterfs-2k5rm gluster peer status
    ```

  配置 Glusterfs 集群为 Kubernetes 的可使用资源

  - 创建服务账户并进行 RBAC 授权

    ```bash
    kubectl create -f rbac.yaml
    ```

  - 创建 Glusterfs-provisioner

    ```bash
    kubectl create -f deployment.yaml
    ```

  - 创建storageclass资源

    ```bash
    #修改 storageclass.yaml 中的 parameters.brickrootPaths 的值，替换为 Glusterfs 节点的IP
    kubectl create -f storageclass.yaml
    ```

  - 创建 pvc 验证

    ```bash
    kubectl create -f pvc.yaml
    kubectl get pvc | grep gluster-simple-claim #创建成功时STATUS为Bound
    ```

  - 创建 pod 验证

    ```bash
    kubectl create -f pod.yaml
    kubectl get po | grep gluster-simple-pod #运行正常时STATUS为Running
    ```

  - 移除验证 pod

    ```bash
    kubectl delete -f pod.yaml
    ```

- ## CentOS Keepalived配置

  借助 **Keepalived** 完成VIP配置

  - 在网关节点安装 Keepalived

  ```bash
  $ yum install -y keepalived
  ```

  - 编辑配置文件

    注意！当前调用健康监测脚本内容为注释状态，原因是在安装 Rainbond 前 需要确保 VIP 已经存在；在 Rainbond 安装完成之后需将注释取消，才能实现健康监测，确保 网关高可用。

    - 主节点

  ```bash
  $ vi /etc/keepalived/keepalived.conf
  
  ! Configuration File for keepalived
  
  global_defs {
     router_id LVS_DEVEL
  }
  #vrrp_script check_gateway {
  # 检测脚本
  #   script "/etc/keepalived/check_gateway_status.sh"
  # 执行间隔时间
  #   interval 5
  #}
  vrrp_instance VI_1 {
                      
      #因使用非抢占模式，这里都为backup
      state BACKUP 
      #网卡设备名，通过 ifconfig 命令确定  
      interface ens6f0     #网卡名称  
      virtual_router_id 51
      #优先级，主节点大于备节点     
      priority 100    
      advert_int 1
      #非抢占模式
      nopreempt
      authentication {
          auth_type PASS
          auth_pass 1111
      }
      virtual_ipaddress {
          <VIP>               
       }
  #     track_script {
  #    check_gateway
  #   }
  }
  ```

  - 从节点

  ```bash
  $ vi /etc/keepalived/keepalived.conf
  
  ! Configuration File for keepalived
  
  global_defs {
     router_id LVS_DEVEL
  }
  #vrrp_script check_gateway {
  # 检测脚本
  #  script "/etc/keepalived/check_gateway_status.sh"
  # 执行间隔时间
  #  interval 5
  #  }    
      vrrp_instance VI_1 {
      #因使用非抢占模式，这里都为backup
      state BACKUP 
      #网卡设备名，通过 ifconfig 命令确定   
      interface ens6f0  #网卡名称
      virtual_router_id 51
      #优先级，主节点大于备节点   
      priority 50
      advert_int 1
      #非抢占模式
      nopreempt
      authentication {
          auth_type PASS
          auth_pass 1111
      }
      virtual_ipaddress {
          <VIP>           
      }
  #     track_script {
  #    check_gateway
  #   }
  }
  ```

  - 健康监测脚本

    扩展对网关节点健康检查的脚本，脚本的功能是当 rbd-gateway 组件停止服务，则关闭本机的 Keepalived，切换 VIP 。(主从都需操作)

  ```bash
  $ vi /etc/keepalived/check_gateway_status.sh 
  
  #!/bin/bash                                                                                             
  /usr/bin/curl -I http://localhost:10254/healthz 
  
  if [ $? -ne 0 ];then
                                                                     
       cat /var/run/keepalived.pid | xargs kill
  
  fi
  ```

  添加执行权限

  ```bash
  $ chmod +x /etc/keepalived/check_gateway_status.sh
  ```

  - 更改 Keepalived systemd 配置文件，添加两项配置

    添加两项参数

  ```bash
  $  vi /lib/systemd/system/keepalived.service
  
  # /lib/systemd/system/keepalived.service
  [Unit]
  Description=Keepalive Daemon (LVS and VRRP)
  After=network-online.target
  Wants=network-online.target
  # Only start if there is a configuration file
  ConditionFileNotEmpty=/etc/keepalived/keepalived.conf
  [Service]
  Type=forking
  KillMode=process
  PIDFile=/var/run/keepalived.pid
  # Read configuration variable file if it is present
  EnvironmentFile=-/etc/default/keepalived
  ExecStart=/usr/sbin/keepalived $DAEMON_ARGS
  ExecReload=/bin/kill -HUP $MAINPID
  # 总是重启该服务
  Restart=always
  # 重启间隔时间
  RestartSec=10
  
  [Install]
  WantedBy=multi-user.target
  ```

  - 启动服务

    启动服务并设置开机自启动

  ```bash
  systemctl start keepalived
  systemctl enable keepalived
  systemctl status keepalived
  ```

  - 检测 VIP 是否启动

  ```bash
  ip a |grep <VIP>
  ```

  在 Rainbond 安装完成之后请将配置文件中注释取消并重启 Keepalived 服务，实现健康监测，以此确保 网关高可用。

  #ubuntu可以参考此文档https://www.rainbond.com/docs/user-operations/Install-extension/ubuntu_keepalived

- ## 部署Rainbond集群

  - 生成所需的 secret 资源

    ```bash
    kubectl create ns rbd-system
    
    kubectl create secret generic rbd-etcd-secret -n rbd-system \
    --from-file=ca-file=/etc/kubernetes/ssl/kube-ca.pem \
    --from-file=cert-file=/etc/kubernetes/ssl/kube-node.pem \
    --from-file=key-file=/etc/kubernetes/ssl/kube-node-key.pem
    #根据实际情况修改license-file的内容
    kubectl create secret generic rbd-console-license -n rbd-system --from-file=license-file=./test2203-goodrain-CONSOLE-LICENSE.yb
    
    kubectl create secret generic rbd-api-license -n rbd-system --from-file=license-file=./test2203-goodrain-LICENSE.yb
    ```

  - 修改 value.yml 文件内容

    根据实际情况修改配置内容，配置项说明参考 https://www.rainbond.com/docs/user-operations/deploy/install-with-helm/vaules-config

  - 安装集群

    ```bash
    git clone https://github.com/goodrain/rainbond-chart.git
    helm install -f value.yml rainbond ./rainbond-chart -n rbd-system
    ```

```
docker run --name=rainbond-allinone --restart=always \
-d -p 7070:7070   \
-v ~/.ssh:/root/.ssh \
-v ~/rainbonddata:/app/data   \
-v ~/license:/opt/license \
-e DISABLE_DEFAULT_APP_MARKET=true \
-e IS_ENTERPRISE_EDITION=true \
-e INSTALL_IMAGE_REPO=goodrain.me \
-e RAINBOND_VERSION=enterprise-2203 \
-e LICENSE_PATH=/opt/license/test2203-goodrain-CONSOLE-LICENSE.yb \
-e LICENSE_KEY=1234567890 \
goodrain.me/rainbond:enterprise-2203-allinone
```

```
mkdir rancher
cat > rancher/clear.sh << EOF
df -h|grep kubelet |awk -F % '{print $2}'|xargs umount 
rm /var/lib/kubelet/* -rf
rm /etc/kubernetes/* -rf
rm /var/lib/rancher/* -rf
rm /var/lib/etcd/* -rf
rm /var/lib/cni/* -rf

rm -rf /var/run/calico 

iptables -F && iptables -t nat -F

ip link del flannel.1

docker ps -a|awk '{print $1}'|xargs docker rm -f
docker volume ls|awk '{print $2}'|xargs docker volume rm

rm -rf /var/etcd/
rm -rf /run/kubernetes/
docker rm -fv $(docker ps -aq)
docker volume rm  $(docker volume ls)
rm -rf /etc/cni
rm -rf /opt/cni

systemctl restart docker
EOF
sh rancher/clear.sh
```

